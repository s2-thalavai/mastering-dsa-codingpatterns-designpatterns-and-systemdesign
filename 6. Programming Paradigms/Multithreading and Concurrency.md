# Multithreading and Concurrency

## 1. Core Concepts

| Concept                         | Description                                                                                 | Example                                              |
| ------------------------------- | ------------------------------------------------------------------------------------------- | ---------------------------------------------------- |
| **Thread**                      | Smallest unit of CPU execution within a process. Shares memory with other threads.          | `Thread` class in Java, `threading.Thread` in Python |
| **Concurrency**                 | Multiple tasks *making progress* at once â€” not necessarily simultaneously.                  | Switching between network I/O and CPU work           |
| **Parallelism**                 | Multiple tasks *executing simultaneously* â€” true simultaneous execution on multi-core CPUs. | Running matrix computations on 8 cores               |
| **Synchronization**             | Coordination between threads to safely access shared resources.                             | Locks, mutexes, semaphores                           |
| **Asynchronous / Non-blocking** | Uses event loops and callbacks rather than blocking threads.                                | `async/await` in JS or Python                        |


## 2. Concurrency vs Multithreading

| Term                 | Nature               | Typical Use                                             |
| -------------------- | -------------------- | ------------------------------------------------------- |
| **Concurrency**      | Structure            | Design pattern to manage multiple tasks effectively     |
| **Multithreading**   | Execution            | Low-level mechanism for true parallel execution         |
| **Asynchronous I/O** | Logical concurrency  | Efficient I/O (not CPU-bound)                           |
| **Multiprocessing**  | Physical parallelism | Bypass Global Interpreter Lock (GIL) or CPU-heavy tasks |

## 3. How Major Modern Languages Handle It

## â˜• Java

- Threads are first-class citizens.

- Executors: thread pools for managing parallel tasks.

- ForkJoinPool / Parallel Streams: high-level concurrency.

- CompletableFuture & Virtual Threads (Java 21):

    - Virtual Threads = lightweight threads (Project Loom).
    
    - Makes asynchronous programming simpler and scalable.

âœ… Modern Java uses virtual threads for massive concurrency with low overhead.

## ğŸ•¸ï¸ JavaScript (Node.js)

- Single-threaded, event-driven architecture.

- Concurrency via Event Loop (non-blocking I/O).

- Worker Threads for CPU-intensive work.

- Promises / async-await syntax for structured concurrency.

âœ… Ideal for I/O-heavy apps, microservices, real-time APIs.


## ğŸ Python

- **Threads:** via threading module (but limited by the GIL)

- **Async:** via asyncio (event loop, async/await)

- **Multiprocessing:** via multiprocessing for CPU-bound tasks

**Framework examples:
**
- FastAPI uses asyncio for high concurrency.

- Celery for parallel background jobs.

âœ… Use async I/O for network-bound, multiprocessing for CPU-bound tasks.

## ğŸ’¨ Go (Golang)

- Goroutines: lightweight threads managed by Go runtime.

- Channels: safe communication between goroutines.

- Scheduler: multiplexes goroutines over OS threads.

âœ… Built-in concurrency model â€” simple, scalable, efficient.

## 4. Patterns & Models

| Pattern                 | Description                               | Example                           |
| ----------------------- | ----------------------------------------- | --------------------------------- |
| **Thread Pool**         | Reuses threads to avoid creation overhead | Java ExecutorService              |
| **Actor Model**         | Isolated units communicate via messages   | Akka (JVM), Orleans (.NET)        |
| **Reactor Pattern**     | Event-driven async I/O                    | Node.js, Spring WebFlux           |
| **Pipeline / Dataflow** | Parallel processing in stages             | Go channels, .NET Dataflow        |
| **Fork/Join**           | Divide tasks recursively                  | Java ForkJoinPool, C++ std::async |

## 5. Common Issues

| Problem             | Description                                 | Mitigation                   |
| ------------------- | ------------------------------------------- | ---------------------------- |
| **Race condition**  | Two threads access shared data concurrently | Use locks, atomic variables  |
| **Deadlock**        | Two threads wait on each otherâ€™s locks      | Lock ordering, timeout locks |
| **Starvation**      | One thread never gets CPU time              | Fair scheduling              |
| **Thread overhead** | Too many threads waste resources            | Use async or thread pools    |

## 6. Modern Trends (2025)

- Structured concurrency (Go, Kotlin, Java Loom): ensures predictable lifetime of concurrent tasks.

- Virtual threads (Java 21+): millions of concurrent tasks.

- Async-first frameworks (FastAPI, Next.js API routes).

- Serverless parallelism (AWS Lambda, Azure Functions).

- Actor-based distributed concurrency (Orleans, Akka).

## 7. When to Use What

| Task Type                 | Best Model                        | Language Example              |
| ------------------------- | --------------------------------- | ----------------------------- |
| CPU-bound computation     | Multithreading or multiprocessing | Java, Go, C++                 |
| I/O-bound tasks           | Async / Event Loop                | Node.js, Python (asyncio)     |
| Real-time processing      | Actor / Reactive                  | Akka, Spring WebFlux          |
| High throughput APIs      | Virtual threads / async I/O       | Java Loom, FastAPI            |
| Distributed microservices | Message queues, Actors            | Kafka, RabbitMQ, Akka Cluster |

---

## 1. concurrency in action across Python, Java, and Go:

> â€œFetch data from multiple URLs concurrently and measure total execution time.â€

Weâ€™ll use **3 different concurrency models â€” async I/O, virtual threads, and goroutines** â€” so you can feel how each modern language approaches concurrency.

## ğŸ 1. Python â€” asyncio (Asynchronous I/O Model)

```python
import asyncio
import aiohttp
import time

urls = [
    "https://example.com",
    "https://httpbin.org/get",
    "https://api.github.com"
]

async def fetch(session, url):
    async with session.get(url) as response:
        return await response.text()

async def main():
    start = time.time()
    async with aiohttp.ClientSession() as session:
        tasks = [fetch(session, url) for url in urls]
        results = await asyncio.gather(*tasks)
        print(f"Fetched {len(results)} URLs in {time.time() - start:.2f}s")

asyncio.run(main())
```

ğŸ§  Concepts Used:

- Event loop (asyncio.run)

- Non-blocking I/O (aiohttp)

- Coroutines (async def)

- await ensures cooperative multitasking.

âœ… Best For: network-heavy workloads (APIs, web crawlers).
âš ï¸ Not truly parallel for CPU-heavy tasks (GIL).

## â˜• 2. Java â€” Virtual Threads (Java 21+)

```java
import java.net.URI;
import java.net.http.*;
import java.time.Duration;
import java.util.List;
import java.util.concurrent.*;

public class VirtualThreadExample {
    public static void main(String[] args) throws Exception {
        var client = HttpClient.newHttpClient();
        var urls = List.of(
            "https://example.com",
            "https://httpbin.org/get",
            "https://api.github.com"
        );

        var start = System.currentTimeMillis();
        try (var executor = Executors.newVirtualThreadPerTaskExecutor()) {
            var futures = urls.stream()
                .map(url -> executor.submit(() -> client.send(
                        HttpRequest.newBuilder(URI.create(url))
                            .timeout(Duration.ofSeconds(10))
                            .build(),
                        HttpResponse.BodyHandlers.ofString()
                )))
                .toList();

            for (var future : futures) {
                System.out.println("Fetched " + future.get().uri());
            }
        }
        System.out.println("Completed in " +
            (System.currentTimeMillis() - start) + " ms");
    }
}
```

ğŸ§  Concepts Used:

- Virtual Threads (Project Loom): ultra-lightweight, millions possible.

- No need for async/await; just normal blocking code.

- Java runtime handles scheduling automatically.

âœ… Best For: scalable APIs, microservices, parallel tasks.
ğŸ’ª True concurrency, not limited like Pythonâ€™s GIL.

## ğŸ¦© 3. Go â€” Goroutines and Channels

```go
package main

import (
    "fmt"
    "io"
    "net/http"
    "sync"
    "time"
)

func fetch(url string, wg *sync.WaitGroup) {
    defer wg.Done()
    resp, err := http.Get(url)
    if err != nil {
        fmt.Println("Error:", err)
        return
    }
    defer resp.Body.Close()
    io.ReadAll(resp.Body) // just to consume
    fmt.Println("Fetched:", url)
}

func main() {
    urls := []string{
        "https://example.com",
        "https://httpbin.org/get",
        "https://api.github.com",
    }

    start := time.Now()
    var wg sync.WaitGroup
    for _, url := range urls {
        wg.Add(1)
        go fetch(url, &wg)
    }
    wg.Wait()
    fmt.Printf("Completed in %.2fs\n", time.Since(start).Seconds())
}
```

ğŸ§  Concepts Used:

- go keyword spawns lightweight goroutines.

- WaitGroup synchronizes them.

- Native concurrency, extremely efficient.

âœ… Best For: concurrent network or compute workloads.
ğŸš€ Can handle 100k+ goroutines easily.

## Performance & Model Comparison

| Language    | Model           | Parallelism | Complexity | Typical Concurrency | Strength                      |
| ----------- | --------------- | ----------- | ---------- | ------------------- | ----------------------------- |
| **Python**  | Async I/O       | Cooperative | Moderate   | 1K+ tasks           | Simplicity for I/O            |
| **Java 21** | Virtual Threads | True        | Low        | Millions            | Easy async replacement        |
| **Go**      | Goroutines      | True        | Very Low   | 100K+               | Lightweight, simple, scalable |

## Summary Visualization

| Task Type                | Best Language    | Why                                        |
| ------------------------ | ---------------- | ------------------------------------------ |
| I/O-bound microservices  | Python (FastAPI) | Async + simple syntax                      |
| Enterprise microservices | Java 21          | Virtual threads scale + stability          |
| Cloud-native systems     | Go               | Fast, minimal memory, concurrency built-in |
| Compute-heavy            | Java / Go        | True parallelism                           |
| Event-driven real-time   | Node.js / Go     | Non-blocking I/O, goroutines               |

---

## 3. move from I/O concurrency â†’ to CPU-bound parallelism.

Weâ€™ll use the same task in all languages:

> â€œFind all prime numbers up to 1,000,000 using multiple workers concurrently.â€

This will show how each language handles real parallel execution (CPU-heavy tasks).

### ğŸ§® Problem Definition

> Find primes â‰¤ 1,000,000 using concurrent workers.

Each worker checks a slice of the range.

## ğŸ 1. Python â€” Multiprocessing (True Parallelism)

ğŸ§  Note: Python threads donâ€™t run CPU-heavy code truly in parallel (because of the GIL),
so we use multiprocessing â€” each process runs on its own CPU core.

```python
from multiprocessing import Pool, cpu_count
import math, time

def is_prime(n):
    if n < 2:
        return False
    for i in range(2, int(math.sqrt(n)) + 1):
        if n % i == 0:
            return False
    return True

def count_primes_in_range(args):
    start, end = args
    return sum(1 for n in range(start, end) if is_prime(n))

if __name__ == "__main__":
    N = 1_000_000
    workers = cpu_count()
    chunk_size = N // workers
    ranges = [(i * chunk_size, (i + 1) * chunk_size) for i in range(workers)]

    start = time.time()
    with Pool(workers) as p:
        total = sum(p.map(count_primes_in_range, ranges))
    print(f"Found {total} primes in {time.time() - start:.2f}s using {workers} processes")
```

âœ… True parallelism (each worker = independent process)
âš ï¸ Overhead from inter-process communication (IPC).

## â˜• 2. Java â€” Virtual Threads or Fixed Thread Pool

ğŸ§  Virtual Threads are more efficient for blocking tasks; for CPU tasks, use a fixed pool tied to CPU cores.

```java
import java.util.concurrent.*;
import java.util.stream.IntStream;

public class ParallelPrimes {
    static boolean isPrime(int n) {
        if (n < 2) return false;
        for (int i = 2; i * i <= n; i++)
            if (n % i == 0) return false;
        return true;
    }

    public static void main(String[] args) throws InterruptedException, ExecutionException {
        int N = 1_000_000;
        int cores = Runtime.getRuntime().availableProcessors();
        ExecutorService executor = Executors.newFixedThreadPool(cores);

        int chunk = N / cores;
        var tasks = IntStream.range(0, cores)
                .mapToObj(i -> {
                    int start = i * chunk;
                    int end = (i + 1) * chunk;
                    return executor.submit(() -> {
                        int count = 0;
                        for (int n = start; n < end; n++)
                            if (isPrime(n)) count++;
                        return count;
                    });
                }).toList();

        long start = System.currentTimeMillis();
        int total = 0;
        for (var f : tasks) total += f.get();
        long duration = System.currentTimeMillis() - start;

        executor.shutdown();
        System.out.printf("Found %d primes in %d ms using %d threads%n", total, duration, cores);
    }
}
```

âœ… True multithreading
ğŸ’ª Scales with available CPU cores
ğŸ§  Java optimizes thread scheduling at OS level.

## ğŸ¦© 3. Go â€” Goroutines + Channels (Simple and Fast)

ğŸ§  Goroutines are very lightweight, and Goâ€™s scheduler automatically spreads them across cores.

```go
package main

import (
    "fmt"
    "math"
    "runtime"
    "sync"
    "time"
)

func isPrime(n int) bool {
    if n < 2 {
        return false
    }
    sqrt := int(math.Sqrt(float64(n)))
    for i := 2; i <= sqrt; i++ {
        if n%i == 0 {
            return false
        }
    }
    return true
}

func countPrimes(start, end int, wg *sync.WaitGroup, result chan<- int) {
    defer wg.Done()
    count := 0
    for n := start; n < end; n++ {
        if isPrime(n) {
            count++
        }
    }
    result <- count
}

func main() {
    N := 1000000
    cores := runtime.NumCPU()
    runtime.GOMAXPROCS(cores)

    chunk := N / cores
    result := make(chan int, cores)
    var wg sync.WaitGroup

    start := time.Now()
    for i := 0; i < cores; i++ {
        wg.Add(1)
        go countPrimes(i*chunk, (i+1)*chunk, &wg, result)
    }

    go func() {
        wg.Wait()
        close(result)
    }()

    total := 0
    for count := range result {
        total += count
    }

    fmt.Printf("Found %d primes in %.2fs using %d cores\n",
        total, time.Since(start).Seconds(), cores)
}
```

âœ… True parallelism with goroutines on multiple OS threads
ğŸ’¨ Lightweight: thousands of goroutines possible
ğŸ§© Synchronization via channels

## Performance Expectation (Rough Benchmarks)

| Language   | Parallel Model  | Real Parallelism | Expected Runtime (1M primes, 8-core) |
| ---------- | --------------- | ---------------- | ------------------------------------ |
| **Python** | Multiprocessing | âœ… Yes            | ~3â€“5s                                |
| **Java**   | Thread Pool     | âœ… Yes            | ~2â€“4s                                |
| **Go**     | Goroutines      | âœ… Yes            | ~1.5â€“3s                              |

(Depends on CPU, memory, and implementation)

## Key Takeaways

| Language   | Concurrency Model      | CPU Parallelism | Simplicity     | Ideal Use                       |
| ---------- | ---------------------- | --------------- | -------------- | ------------------------------- |
| **Python** | Multiprocessing        | âœ…               | âš ï¸ Moderate    | Data science, background jobs   |
| **Java**   | Thread Pool / ForkJoin | âœ…               | âš™ï¸ Medium      | Enterprise compute workloads    |
| **Go**     | Goroutines             | âœ…âœ…              | ğŸ§© Very simple | Cloud-native concurrent systems |

---

## local concurrency â†’ to distributed parallelism

moving from local concurrency â†’ to distributed parallelism â€” where multiple nodes (servers) work together to 
solve CPU-intensive or data-intensive problems in parallel.

Letâ€™s keep the same conceptual problem â€” â€œcount primes up to Nâ€ â€” and scale it across a cluster using modern distributed concurrency models.

### ğŸŒ 1. The Shift: Concurrency â†’ Distributed Parallelism

| Concept            | Local (Single Machine)          | Distributed (Cluster)                            |
| ------------------ | ------------------------------- | ------------------------------------------------ |
| **Parallel Units** | Threads / Processes             | Nodes / Containers                               |
| **Communication**  | Shared memory / message passing | Network (HTTP, gRPC, MQ, Kafka)                  |
| **Scheduling**     | OS / language runtime           | Distributed orchestrator (K8s, Ray, Spark, Dask) |
| **Goal**           | Maximize CPU core usage         | Maximize all machinesâ€™ cores                     |

### ğŸš€ 2. Three Distributed Approaches

Weâ€™ll cover:

- Python (Ray / Dask)

- Java (Akka / gRPC microservices)

- Go (Worker Pool + Message Queue)

## ğŸ A. Python â€” Using Ray (Distributed Task Execution)

Ray provides a Python-native distributed execution framework
â†’ think of it as â€œmultiprocessing across many machinesâ€.

```python
import ray
import math, time

# Start Ray cluster (or connect to existing one)
ray.init(address="auto")  # or ray.init() for local

@ray.remote
def count_primes(start, end):
    def is_prime(n):
        if n < 2: return False
        for i in range(2, int(math.sqrt(n)) + 1):
            if n % i == 0: return False
        return True
    return sum(1 for n in range(start, end) if is_prime(n))

if __name__ == "__main__":
    N = 10_000_000
    num_workers = 20
    chunk = N // num_workers

    start = time.time()
    tasks = [count_primes.remote(i*chunk, (i+1)*chunk) for i in range(num_workers)]
    results = ray.get(tasks)
    total = sum(results)
    print(f"Found {total} primes in {time.time() - start:.2f}s using {num_workers} Ray workers")
```

âœ… Distributed computation (each task may run on a different machine)
âš™ï¸ Cluster orchestration built in
ğŸ’¡ Easy scaling â€” just add more Ray nodes.

## â˜• B. Java â€” Akka Cluster (Actor Model)

Akka lets you create distributed actors that process messages independently across nodes.
Perfect for massive parallelism + fault tolerance.

ğŸ§© Cluster Architecture

- Master actor â†’ divides work into chunks.

- Worker actors â†’ deployed on multiple JVM nodes.

- Result collector â†’ aggregates counts.

```scala
ğŸ’» Example (simplified)
// Scala code (Akka)
import akka.actor._

case class CountPrimes(start: Int, end: Int)
case class Result(count: Int)

class Worker extends Actor {
  def isPrime(n: Int) = n > 1 && (2 to math.sqrt(n).toInt).forall(n % _ != 0)

  def receive = {
    case CountPrimes(start, end) =>
      val count = (start until end).count(isPrime)
      sender() ! Result(count)
  }
}

class Master(numWorkers: Int, range: Int) extends Actor {
  val chunk = range / numWorkers
  val workers = (1 to numWorkers).map(_ => context.actorOf(Props[Worker]))
  var results = 0
  var pending = numWorkers

  override def preStart(): Unit = {
    workers.zipWithIndex.foreach { case (w, i) =>
      w ! CountPrimes(i * chunk, (i + 1) * chunk)
    }
  }

  def receive = {
    case Result(c) =>
      results += c
      pending -= 1
      if (pending == 0) {
        println(s"Total primes: $results")
        context.system.terminate()
      }
  }
}

object ClusterMain extends App {
  val system = ActorSystem("PrimeCluster")
  system.actorOf(Props(new Master(numWorkers = 8, range = 1_000_000)))
}
```

âœ… Distributed actor system (via Akka Cluster)
ğŸ’ª Resilient, message-driven, fault-tolerant
âš™ï¸ Ideal for enterprise distributed workloads

## ğŸ¦© C. Go â€” Worker Pool + Message Queue (RabbitMQ or NATS)

Go excels at simple, networked concurrency, so itâ€™s perfect for distributed worker systems.

ğŸ§© Architecture

Master Service sends tasks (ranges) via message queue.

Workers subscribe, compute prime counts, and publish results.

ğŸ§  Master (Go + RabbitMQ)

```go
package main

import (
    "fmt"
    "log"
    "github.com/streadway/amqp"
)

func main() {
    conn, _ := amqp.Dial("amqp://guest:guest@localhost/")
    ch, _ := conn.Channel()
    q, _ := ch.QueueDeclare("tasks", false, false, false, false, nil)

    N := 1000000
    workers := 10
    chunk := N / workers

    for i := 0; i < workers; i++ {
        body := fmt.Sprintf("%d,%d", i*chunk, (i+1)*chunk)
        ch.Publish("", q.Name, false, false, amqp.Publishing{Body: []byte(body)})
    }

    fmt.Println("Distributed tasks to queue.")
}

ğŸ§© Worker (runs on any node)
package main

import (
    "fmt"
    "log"
    "math"
    "strconv"
    "strings"
    "github.com/streadway/amqp"
)

func isPrime(n int) bool {
    if n < 2 { return false }
    sqrt := int(math.Sqrt(float64(n)))
    for i := 2; i <= sqrt; i++ {
        if n%i == 0 { return false }
    }
    return true
}

func main() {
    conn, _ := amqp.Dial("amqp://guest:guest@localhost/")
    ch, _ := conn.Channel()
    msgs, _ := ch.Consume("tasks", "", true, false, false, false, nil)

    for msg := range msgs {
        parts := strings.Split(string(msg.Body), ",")
        start, _ := strconv.Atoi(parts[0])
        end, _ := strconv.Atoi(parts[1])

        count := 0
        for n := start; n < end; n++ {
            if isPrime(n) { count++ }
        }
        fmt.Printf("Worker processed %dâ€“%d, primes=%d\n", start, end, count)
    }
}
```

âœ… Horizontally scalable â€” add more workers on any node
âœ… Decoupled via RabbitMQ / NATS / Kafka
ğŸ’ª Ideal for microservices or distributed compute grids.

## 4. Comparison â€” Distributed Concurrency

| Language            | Model                            | Communication       | Scaling            | Best For                       |
| ------------------- | -------------------------------- | ------------------- | ------------------ | ------------------------------ |
| **Python (Ray)**    | Task-based distributed execution | gRPC (Ray internal) | Linear             | ML, data pipelines, simulation |
| **Java (Akka)**     | Actor-based message passing      | Akka Cluster        | Linear + resilient | Event-driven microservices     |
| **Go (MQ Workers)** | Pub/Sub worker pool              | RabbitMQ / Kafka    | Linear             | Cloud microservices, pipelines |

## ğŸ§  6. Advanced Distributed Models (2025)

| Framework                      | Language       | Highlights                             |
| ------------------------------ | -------------- | -------------------------------------- |
| **Ray / Dask**                 | Python         | Cluster compute made easy              |
| **Akka / Quarkus / Micronaut** | Java           | Reactive microservices + actors        |
| **Temporal.io**                | Go / Java / TS | Workflow orchestration                 |
| **Celery + Redis**             | Python         | Task queue for async distributed jobs  |
| **Kafka Streams / Flink**      | JVM            | Streaming computation                  |
| **KEDA + RabbitMQ / NATS**     | Any            | Autoscaling distributed task executors |

## Summary

| Level   | Tool / Model                | Core Concept                | Example                 |
| ------- | --------------------------- | --------------------------- | ----------------------- |
| Local   | Threads / Goroutines        | Parallel tasks on 1 machine | CPU-bound concurrency   |
| Cluster | Actors / Workers / Tasks    | Distributed parallel tasks  | Multi-node compute grid |
| Global  | Message queues / Kubernetes | Event-driven autoscaling    | Cloud-native scaling    |

---

# Distributed concurrency in a Procure-to-Pay (P2P) example â€” end-to-end design (Spring Boot + FastAPI + Go)

Nice â€” hereâ€™s a full, practical blueprint you can use to build a distributed, concurrent P2P pipeline that handles invoice ingest 
â†’ OCR/document analysis â†’ rule-engine validation â†’ enrichment â†’ posting/archival with horizontally-scalable microservices in Spring Boot (Java),
FastAPI (Python), and Go. Iâ€™ll include architecture, message flows, sample contracts, concise code snippets for each microservice, 
k8s deployment patterns, observability, scaling, error/Retry patterns and security.

## 1) High-level architecture (text diagram)

```java
[Client / ERP] --> API Gateway (Ingress) --> Invoice Ingest Service (FastAPI)
        |
        v
   Message Broker (Kafka/RabbitMQ)  <--->  OCR Service (Go)  <--->  Rule Engine (Spring Boot)
        |                                   |                     |
        v                                   v                     v
   Enrichment Service (Python/Go)      ML/Document DB         Result Collector / DB
        |
        v
   Accounting System (ERP) / Audit log
```

## Key ideas:

- **Async, event-driven:** Message broker as backbone (Kafka for high throughput / exactly-once-ish, RabbitMQ for simpler queues).

- **Microservice roles:** Ingest (HTTP), OCR (CPU + GPU friendly), Rule Engine (stateful business rules), Enricher (lookups), Result Collector (aggregate & persist).

- **Autoscale workers:** KEDA or HPA scale consumers based on queue lag / CPU.

## 2) Message model (JSON) â€” example contract

invoice.uploaded (produced by Invoice Ingest)

```json
{
  "invoice_id": "INV-2025-0001",
  "vendor_id": "V-420",
  "origin": "erp.systemA",
  "uploaded_at": "2025-10-29T07:12:31Z",
  "s3_uri": "s3://invoices-bucket/2025/10/INV-2025-0001.pdf",
  "metadata": {
    "amount": 2543.75,
    "currency": "USD",
    "invoice_date": "2025-10-25"
  }
}
```

invoice.ocr.completed

```json
{
  "invoice_id": "INV-2025-0001",
  "lines": [
    {"desc":"Widget","qty":2,"unit_price":1000.0}
  ],
  "supplier": {"name":"Acme", "tax_id":"12345"},
  "total": 2543.75,
  "ocr_confidence": 0.93
}
```

invoice.validation.result

```json
{
  "invoice_id":"INV-2025-0001",
  "status":"REJECTED",
  "violations":[
    {"rule_id":"R-13","message":"Tax mismatch","severity":"HIGH"}
  ],
  "timestamp":"2025-10-29T07:13:20Z"
}
```

Design note: small, immutable messages with clear versioning fields (schema_version) so services can evolve.

## 3) Message broker choices & topics/queues

- Kafka (recommended if high throughput & ordering matters)

    - Topics: invoice.uploaded, invoice.ocr, invoice.ocr.completed, invoice.validation.request, invoice.validation.result, invoice.enriched, invoice.audit
    
    - Partitioning: partition by invoice_id for ordering

- RabbitMQ / NATS (if simpler semantics)

    - Queues per consumer group, use dead-letter exchange for poison messages
 
## 4) Service responsibilities + short code sketches

A. Invoice Ingest â€” FastAPI (Python)

- Receives HTTP POST with file or S3 pointer

- Stores file to object-store and publishes invoice.uploaded to broker

- Light-weight, scale via replicas

Minimal snippet (publishing to Kafka with aiokafka):

```python
# ingest_service.py
from fastapi import FastAPI, UploadFile
from aiokafka import AIOKafkaProducer
import asyncio, json, uuid, datetime

app = FastAPI()
producer = None

@app.on_event("startup")
async def startup():
    global producer
    producer = AIOKafkaProducer(bootstrap_servers="kafka:9092")
    await producer.start()

@app.post("/upload")
async def upload(file: UploadFile):
    invoice_id = f"INV-{uuid.uuid4()}"
    # save to S3 (pseudo)
    s3_uri = f"s3://invoices/{invoice_id}.pdf"
    # publish event
    msg = {
        "invoice_id": invoice_id,
        "uploaded_at": datetime.datetime.utcnow().isoformat() + "Z",
        "s3_uri": s3_uri
    }
    await producer.send_and_wait("invoice.uploaded", json.dumps(msg).encode())
    return {"invoice_id": invoice_id}
```

**Concurrency:** FastAPI + Uvicorn workers (async) handle many concurrent uploads; use S3 presigned URLs for large files.

## B. OCR Service â€” Go (CPU-bound, parallel)

- Pulls invoice.uploaded tasks, downloads PDF from S3, runs OCR (Tesseract or cloud OCR), publishes invoice.ocr.completed.

- Use goroutines to parallelize pages; limit concurrency per worker by semaphore.

### Minimal (consumer outline):

```go
// consumer.go (pseudo)
func worker(msg InvoiceUploaded) {
    // download from S3
    // use OCR engine with worker-pool for pages
    // parse structured fields => ocrResult
    publish("invoice.ocr.completed", ocrResult)
}
```

**Concurrency:** each pod can run multiple goroutines; scale pods horizontally.

## C. Rule Engine â€” Spring Boot (Java)

- Subscribes to invoice.ocr.completed or invoice.validation.request

- Executes rules (Drools / custom engine). Could be stateful: hold past supplier flags, blacklists

- Emits invoice.validation.result and optionally triggers human review if severe.

Spring Boot + Kafka snippet (consumer + processing):

```java
// KafkaListener
@KafkaListener(topics = "invoice.ocr.completed")
public void onMessage(String payload) {
    InvoiceOcr ocr = objectMapper.readValue(payload, InvoiceOcr.class);
    ValidationResult result = ruleEngine.evaluate(ocr);
    kafkaTemplate.send("invoice.validation.result", objectMapper.writeValueAsString(result));
}
```

**Scaling:** use fixed thread pools sized to CPU cores to process rules concurrently. 
For complex rules consider a rule-service cluster with sticky routing when rule state matters.

## D. Enrichment Service â€” Python/Go

- Looks up vendor data, tax rates, payment terms (from DB or external APIs)

- Adds enrichment and publishes invoice.enriched or writes to DB

## E. Result Collector / Database

Consume invoice.validation.result and persist to transactional DB (Postgres)

Push notifications to UI / ERP or create work item in downstream systems

Writes audit trail to immutable store (S3 + event store/table)

## 5) Autoscaling & orchestration (Kubernetes + KEDA)

Suggested pattern:

- Deploy each microservice as a Deployment with HorizontalPodAutoscaler (HPA).

- Use KEDA to autoscale consumers by Kafka partition lag or RabbitMQ queue length.

- For CPU-bound OCR, scale by CPU usage and set PodDisruptionBudgets.

Example: scale OCR workers when invoice.uploaded lag > threshold. Set max pods to cap costs.

## 6) Error handling, retries & dead-letter

- At-least-once processing: design idempotency (use invoice_id) to avoid double-processing.

- Retries: exponential backoff for transient errors (3-5 attempts), then send to dead-letter topic with failure reason.

- Poison messages: send to DLQ and create human alert/work item.

- Transactional writes: use Kafka transactions where available (producer tx) when you must atomically publish events + DB writes.

## 7) Observability & tracing

- Metrics: Prometheus exporters on each service (export HTTP request latency, processing time per event, queue lag).

- Dashboards: Grafana dashboards for throughput, error rate, consumer lag.

- Distributed Tracing: Jaeger / OpenTelemetry for cross-service traces (span per ingest â†’ ocr â†’ rule).

- Logs: structured JSON logs shipped to ELK or Loki; include invoice_id as correlation id.

- SLOs: e.g., 95th percentile end-to-end processing < 30s, mean OCR CPU < X.

- Correlation: set trace_id and include in Kafka message headers for tracing across services.

## 8) Security & compliance

- Auth: API Gateway enforces OAuth2/JWT; only authenticated clients may POST invoices.

- mTLS: between services (or service mesh like Istio) for in-cluster encryption and mutual auth.

- Data-at-rest: S3 encryption, DB encryption (TDE).

- PII: redact or secure PII in logs; store sensitive fields encrypted.

- Audit: write immutable audit events to S3 and append-only DB table for regulatory compliance.

- Secrets: use Kubernetes Secrets or external store (HashiCorp Vault) for DB keys, broker creds.

## 9) Performance tuning & recommended defaults

- OCR: heavy CPU â€” pin to nodes with GPUs if using GPU OCR; set node taints/affinities.

- Rule Engine: run rules in memory; shard by vendor group if rules are heavy and stateful.

- Message sizes: keep minimal; donâ€™t pass PDFs in Kafkaâ€”use S3 pointer.

- Partitioning: partition Kafka topics by vendor_id or invoice_id depending on ordering needs.

- Idempotency: use DB unique constraints on invoice_id to prevent double writes.

## 10) DevOps & CI/CD

- Use GitOps (ArgoCD) or Helm for deployments.

- Pipeline stages:

    - build â†’ unit tests â†’ container image â†’ security scans (Snyk/Trivy) â†’ push to registry â†’ deploy to staging â†’ run integration tests (including contract tests for Kafka topics) â†’ promote to prod.

- Canary or blue/green deployment for rule engine changes (rules can break workflows).

## 11) Example end-to-end sequence (flow)

- Client uploads invoice to API â†’ API writes file to S3 and publishes invoice.uploaded.

- OCR worker consumes invoice.uploaded, downloads PDF, OCRs, publishes invoice.ocr.completed.

- Rule Engine consumes invoice.ocr.completed, runs validations; triggers invoice.validation.result.

- Enrichment service responds to validation result and calls external lookups if needed.

- Result Collector persists final status and notifies ERP via REST/gRPC.

- Audit events stored into S3; monitoring dashboards updated.

## 12) Minimal k8s snippet â€” OCR worker (Deployment + HPA)

```yml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ocr-worker
spec:
  replicas: 2
  template:
    spec:
      containers:
      - name: ocr
        image: registry.example.com/ocr:latest
        resources:
          limits:
            cpu: "2"
            memory: "4Gi"
          requests:
            cpu: "1"
            memory: "2Gi"
---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: ocr-worker-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: ocr-worker
  minReplicas: 2
  maxReplicas: 12
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
```

Use KEDA to autoscale by queue lag for Kafka/RabbitMQ.

## 13) Testing & contracts

- Contract tests for JSON messages (Pact or similar).

- Chaos testing: simulate broker outages, consumer crashes, worker slowdowns.

- Performance tests: use k6 / locust to simulate high ingest rates and measure consumer scaling.

## 14) Tradeoffs & patterns

- Synchronous REST orchestration is simpler but not scalable for heavy loads â€” avoid it for OCR & rules.

- Event-driven favors loose coupling, resilience, and scaling but increases system complexity.

- Exactly-once semantics are hard; aim for idempotency + at-least-once.

- Stateful rules: if rules depend on historical state, keep that state in a fast store (Redis/DB) and shard accordingly.

## 15) Next steps I can do for you (pick any â€” Iâ€™ll produce right away)

- Full sample repo scaffold (FastAPI ingest, Go OCR consumer, Spring Boot rule engine) with Dockerfiles.

- Kubernetes manifests + KEDA autoscaling + Helm charts for the whole stack.

- Complete Kafka message schemas (Avro/Protobuf) and contract tests.

- Detailed Prometheus/Grafana dashboards and OpenTelemetry instrumented code snippets.

- Example Drools rules and tests for common invoice checks.

